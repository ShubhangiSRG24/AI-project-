# -*- coding: utf-8 -*-
"""Liver disease prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16VgiREdSJCq4BPMBo0yv9EXAjed_6saE
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings

warnings.simplefilter("ignore")

liver_df = pd.read_csv("/content/indian_liver_patient.csv")

# Total number of columns in the dataset
liver_df.columns

liver_df.head()

# Information about the dataset
liver_df.info()

# Checking if there is some null values or not
liver_df.isnull().sum()

liver_df["Albumin_and_Globulin_Ratio"].mean()

liver_df["Albumin_and_Globulin_Ratio"] = liver_df["Albumin_and_Globulin_Ratio"].fillna(liver_df["Albumin_and_Globulin_Ratio"].mean())

liver_df.isnull().sum()

# Dropping rows containing null values
#liver_df=liver_df.dropna()

liver_df

liver_df.duplicated().sum()

# Removing duplicates in dataset
liver_df = liver_df.drop_duplicates(subset = None, keep = 'first')

"""## Remove Outliers from the features"""

liver_df.drop(liver_df[liver_df['Total_Bilirubin']>50].index, inplace = True)
liver_df.drop(liver_df[liver_df['Direct_Bilirubin']>15].index, inplace = True)
liver_df.drop(liver_df[liver_df['Alamine_Aminotransferase']>1500].index, inplace = True)
liver_df.drop(liver_df[liver_df['Aspartate_Aminotransferase']>2000].index, inplace = True)
print(liver_df.shape)

sns.countplot(data=liver_df, x = 'Dataset', label='Count')

LD, NLD = liver_df['Dataset'].value_counts()
print('Number of patients diagnosed with liver disease: ',LD)
print('Number of patients not diagnosed with liver disease: ',NLD)

liver_df.describe()

# convert dataset row into 0 and 1
def partition(x):
    if x == 2:
        return 0
    return 1

liver_df['Dataset'] = liver_df['Dataset'].map(partition)

sns.countplot(data=liver_df, x = 'Gender', label='Count')

M, F = liver_df['Gender'].value_counts()
print('Number of patients that are male: ',M)
print('Number of patients that are female: ',F)

def partition(x):
    if x =='Male':
        return 0
    return 1

liver_df['Gender'] = liver_df['Gender'].map(partition)

liver_df

correlation_matrix =liver_df.corr()
f, ax = plt.subplots(figsize=(10, 8))
sns.heatmap(correlation_matrix, mask=np.zeros_like(correlation_matrix, dtype=np.bool),
            cmap=sns.diverging_palette(220, 10, as_cmap=True),
            square=True, ax=ax)
correlation_matrix['Dataset'].sort_values(ascending=False)

"""## Data Preparation"""

# Object for target variable
y = liver_df.Dataset
# Object for input features
# X = liver_df.drop(['Dataset','Total_Protiens','Gender','Albumin','Albumin_and_Globulin_Ratio'], axis=1)
X = liver_df.drop(['Dataset'], axis=1)

# Split Xand y into train and test sets
from sklearn.model_selection import train_test_split
X1_train, X1_test, y1_train, y1_test = train_test_split(X, y, 
                                                    test_size=0.2, 
                                                    random_state=1234)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X1_train = sc.fit_transform(X1_train)
X1_test = sc.transform(X1_test)

"""### Oversampling"""

noLiverDisease = liver_df[liver_df['Dataset']==0]
hasLiverDisease = liver_df[liver_df['Dataset']==1]

print(noLiverDisease.shape, hasLiverDisease.shape)

from imblearn.over_sampling import RandomOverSampler
op = RandomOverSampler(random_state=30)
X_res, y_res = op.fit_resample(X,y)

X_res.shape, y_res.shape

# Split Xand y into train and test sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, 
                                                    test_size=0.2, 
                                                    random_state=1234)

# Print number of observations in the above sets
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)

"""## Feature Scaling"""

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

"""## Using Machine Learning Models

### Logistic Regression
"""

from sklearn.linear_model import LogisticRegression
log_classifier = LogisticRegression(random_state = 0)
log_classifier.fit(X1_train, y1_train)

log_y1_pred = log_classifier.predict(X1_test)

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y1_test, log_y1_pred)
sns.heatmap(cm, annot = True)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score


print("The accuracy is : " )
print(accuracy_score(y1_test, log_y1_pred))
print("The precision is : " )
print(precision_score(y1_test, log_y1_pred))
print("The recall is : " )
print(recall_score(y1_test, log_y1_pred))
print("The f1_score is : " )
print(f1_score(y1_test, log_y1_pred))

from sklearn.linear_model import LogisticRegression
log_classifier = LogisticRegression(random_state = 0)
log_classifier.fit(X_train, y_train)

log_y_pred = log_classifier.predict(X_test)

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, log_y_pred)
sns.heatmap(cm, annot = True)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

acc_log = accuracy_score(y_test, log_y_pred)
print("The accuracy is : " )
print(accuracy_score(y_test, log_y_pred))
print("The precision is : " )
print(precision_score(y_test, log_y_pred))
print("The recall is : " )
print(recall_score(y_test, log_y_pred))
print("The f1_score is : " )
print(f1_score(y_test, log_y_pred))

from sklearn.ensemble import AdaBoostClassifier

ada_log = AdaBoostClassifier(LogisticRegression(), n_estimators=700, algorithm="SAMME", learning_rate=0.5, random_state= 30)
ada_log.fit(X_train, y_train)
y_predadalog = ada_log.predict(X_test)
print(accuracy_score(y_test, y_predadalog))

"""### KNN algorithm"""

from sklearn.neighbors import KNeighborsClassifier
knn_classifier = KNeighborsClassifier(n_neighbors=75, metric = 'minkowski')
knn_classifier.fit(X1_train, y1_train) 

knn_y1_pred = knn_classifier.predict(X1_test)

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y1_test, knn_y1_pred)
sns.heatmap(cm, annot = True)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

print("The accuracy is : " )
print(accuracy_score(y1_test, knn_y1_pred))
print("The precision is : " )
print(precision_score(y1_test, knn_y1_pred))
print("The recall is : " )
print(recall_score(y1_test, knn_y1_pred))
print("The f1_score is : " )
print(f1_score(y1_test, knn_y1_pred))

from sklearn.neighbors import KNeighborsClassifier
knn_classifier = KNeighborsClassifier(n_neighbors=75, metric = 'minkowski')
knn_classifier.fit(X_train, y_train)

knn_y_pred = knn_classifier.predict(X_test)

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, knn_y_pred)
sns.heatmap(cm, annot = True)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
acc_knn = accuracy_score(y_test, knn_y_pred)
print("The accuracy is : " )
print(accuracy_score(y_test, knn_y_pred))
print("The precision is : " )
print(precision_score(y_test, knn_y_pred))
print("The recall is : " )
print(recall_score(y_test, knn_y_pred))
print("The f1_score is : " )
print(f1_score(y_test, knn_y_pred))

"""## SVM Algorithm"""

from sklearn.svm import SVC
svm_classifier = SVC(kernel = 'rbf', random_state = 1)
svm_classifier.fit(X1_train, y1_train)

svm_y1_pred = svm_classifier.predict(X1_test)

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y1_test, svm_y1_pred)
sns.heatmap(cm, annot = True)

from sklearn.metrics import accuracy_score, precision_score

print("The accuracy is : " )
print(accuracy_score(y1_test, svm_y1_pred))
print("The precision is : " )
print(precision_score(y1_test, svm_y1_pred))
print("The recall is : " )
print(recall_score(y1_test, svm_y1_pred))
print("The f1_score is : " )
print(f1_score(y1_test, svm_y1_pred))

from sklearn.svm import SVC
svm_classifier = SVC(kernel = 'rbf', random_state = 1)
svm_classifier.fit(X_train, y_train)

svm_y_pred = svm_classifier.predict(X_test)

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, svm_y_pred)
sns.heatmap(cm, annot = True)

from sklearn.metrics import accuracy_score, precision_score
acc_svm = accuracy_score(y_test, svm_y_pred)
print("The accuracy is : " )
print(accuracy_score(y_test, svm_y_pred))
print("The precision is : " )
print(precision_score(y_test, svm_y_pred))
print("The recall is : " )
print(recall_score(y_test, svm_y_pred))
print("The f1_score is : " )
print(f1_score(y_test, svm_y_pred))

"""## Random Forest classifier"""

from sklearn.ensemble import RandomForestClassifier
import warnings
warnings.filterwarnings('ignore')

r1_classifier = RandomForestClassifier(n_estimators=80,criterion= 'gini', random_state=10, max_features='sqrt')
model = r1_classifier.fit(X1_train, y1_train)
y1_pred_random = model.predict(X1_test)

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y1_test, y1_pred_random)
sns.heatmap(cm, annot = True)

from sklearn.metrics import accuracy_score, precision_score

print("The accuracy is : " )
print(accuracy_score(y1_test, y1_pred_random))
print("The precision is : " )
print(precision_score(y1_test, y1_pred_random))
print("The recall is : " )
print(recall_score(y1_test, y1_pred_random))
print("The f1_score is : " )
print(f1_score(y1_test, y1_pred_random))

from sklearn.ensemble import RandomForestClassifier
import warnings
warnings.filterwarnings('ignore')

r_classifier = RandomForestClassifier(n_estimators=80,criterion= 'gini', random_state=10, max_features='sqrt')
model = r_classifier.fit(X_train, y_train)
y_pred_random = model.predict(X_test)

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred_random)
sns.heatmap(cm, annot = True)

from sklearn.metrics import accuracy_score, precision_score
acc_ran = accuracy_score(y_test, y_pred_random)
print("The accuracy is : " )
print(accuracy_score(y_test, y_pred_random))
print("The precision is : " )
print(precision_score(y_test, y_pred_random))
print("The recall is : " )
print(recall_score(y_test, y_pred_random))
print("The f1_score is : " )
print(f1_score(y_test, y_pred_random))

from sklearn.ensemble import AdaBoostClassifier

ada_log = AdaBoostClassifier(base_estimator = RandomForestClassifier(random_state = 90), n_estimators=100, algorithm="SAMME", learning_rate=0.5, random_state= 95)
ada_log.fit(X_train, y_train)
y_predadalog = ada_log.predict(X_test)
print(accuracy_score(y_test, y_predadalog))

from sklearn.ensemble import GradientBoostingClassifier
gbclass = GradientBoostingClassifier(
                    random_state = 1000,
                    verbose = 0,
                    n_estimators = 10,
                    learning_rate = 0.9,
                    loss = 'deviance',
                    max_depth = 3
                   )
# Train the model using the training sets and check score
gbclass.fit(X_train, y_train)
#Predict Output
predicted= gbclass.predict(X_test)

gbclass_score = round(gbclass.score(X_train, y_train) * 100, 2)
gbclass_score_test = round(gbclass.score(X_test, y_test) * 100, 2)
print('Score: \n', gbclass_score)
print('Test Score: \n', gbclass_score_test)
print('Accuracy: \n', accuracy_score(y_test,predicted))
print(confusion_matrix(predicted,y_test))

"""## Comparing Model performance"""

models_comparison = [['Logistic Regression',acc_log*100],
                     ['K Nearest Neighbor',acc_knn*100],
                     ['Support Vector Classfication',acc_svm*100], 
                     ['Random Forest Classifiaction',acc_ran*100]
                    ]
models_compaison_df = pd.DataFrame(models_comparison,columns=['Model','% Accuracy'])
models_compaison_df.head()

fig = plt.figure(figsize=(20,8))
sns.set()
sns.barplot(x='Model',y='% Accuracy',data=models_compaison_df,palette='Dark2')
plt.xticks(size=18)
plt.ylabel('% Accuracy',size=14)
plt.xlabel('Model',size=14)

liver_df

pip install sdv

import pandas as pd
data = pd.read_csv('/content/indian_liver_patient.csv')
data.head()

from sdv.tabular import GaussianCopula
model = GaussianCopula()
#from sdv.tabular import CTGAN
#model = CTGAN()
model.fit(data)
sample = model.sample(8000)
sample.head()
model = GaussianCopula(primary_key='Alkaline_Phosphotase')
model.fit(data)

from sdv.evaluation import evaluate
evaluate(sample, data, metrics=['CSTest', 'KSTest'], aggregate=False)

sample.shape

sample.head

sample.isnull().sum()

sample.duplicated().sum()

sample.drop(sample[sample['Total_Bilirubin']>50].index, inplace = True)
sample.drop(sample[sample['Direct_Bilirubin']>15].index, inplace = True)
sample.drop(sample[sample['Alamine_Aminotransferase']>1500].index, inplace = True)
sample.drop(sample[sample['Aspartate_Aminotransferase']>2000].index, inplace = True)
print(sample.shape)

import seaborn as sns
sns.countplot(data=sample, x = 'Dataset', label='Count')

LD, NLD = sample['Dataset'].value_counts()
print('Number of patients diagnosed with liver disease: ',LD)
print('Number of patients not diagnosed with liver disease: ',NLD)

# convert dataset row into 0 and 1
def partition(x):
    if x == 2:
        return 0
    return 1

sample['Dataset'] = sample['Dataset'].map(partition)

sns.countplot(data=sample, x = 'Gender', label='Count')

M, F = sample['Gender'].value_counts()
print('Number of patients that are male: ',M)
print('Number of patients that are female: ',F)

def partition(x):
    if x =='Male':
        return 0
    return 1

sample['Gender'] = sample['Gender'].map(partition)

sample

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
correlation_matrix =sample.corr()
f, ax = plt.subplots(figsize=(10, 8))
sns.heatmap(correlation_matrix, mask=np.zeros_like(correlation_matrix, dtype=np.bool),
            cmap=sns.diverging_palette(220, 10, as_cmap=True),
            square=True, ax=ax)
correlation_matrix['Dataset'].sort_values(ascending=False)

"""##Data Preparation

"""

# Object for target variable
ys = sample.Dataset
# Object for input features
# X = liver_df.drop(['Dataset','Total_Protiens','Gender','Albumin','Albumin_and_Globulin_Ratio'], axis=1)
Xs = sample.drop(['Dataset', 'Gender','Total_Protiens','Gender','Albumin','Albumin_and_Globulin_Ratio'], axis=1)

# Split Xand y into train and test sets
from sklearn.model_selection import train_test_split
Xs1_train, Xs1_test, ys1_train, ys1_test = train_test_split(Xs, ys, 
                                                    test_size=0.5, 
                                                    random_state=1884)

"""## Feature scaling of synthetic data"""

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
Xs1_train = sc.fit_transform(Xs1_train)
Xs1_test = sc.transform(Xs1_test)

from sklearn.ensemble import RandomForestClassifier
import warnings
warnings.filterwarnings('ignore')

r1_classifier = RandomForestClassifier(n_estimators=80,criterion= 'gini', random_state=10, max_features='sqrt')
model = r1_classifier.fit(Xs1_train, ys1_train)
ys1_pred_random = model.predict(Xs1_test)

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(ys1_test, ys1_pred_random)
sns.heatmap(cm, annot = True)

from sklearn.metrics import accuracy_score, precision_score, recall_score,f1_score

print("The accuracy is : " )
print(accuracy_score(ys1_test, ys1_pred_random))
print("The precision is : " )
print(precision_score(ys1_test, ys1_pred_random))
print("The recall is : " )
print(recall_score(ys1_test, ys1_pred_random))
print("The f1_score is : " )
print(f1_score(ys1_test, ys1_pred_random))

from sklearn.svm import SVC
svm_classifier = SVC(kernel = 'rbf', random_state = 1)
svm_classifier.fit(Xs1_train, ys1_train) 
svm1_y_pred = svm_classifier.predict(Xs1_test)
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(ys1_test, svm1_y_pred)
sns.heatmap(cm, annot = True)

from sklearn.metrics import accuracy_score, precision_score
acc_svm = accuracy_score(ys1_test, svm1_y_pred)
print("The accuracy is : " )
print(accuracy_score(ys1_test, svm1_y_pred))
print("The precision is : " )
print(precision_score(ys1_test, svm1_y_pred))
print("The recall is : " )
print(recall_score(ys1_test, svm1_y_pred))
print("The f1_score is : " )
print(f1_score(ys1_test, svm1_y_pred))

from sklearn.ensemble import AdaBoostClassifier

ada_log = AdaBoostClassifier(base_estimator = RandomForestClassifier(random_state = 90), n_estimators=100, algorithm="SAMME", learning_rate=0.5, random_state= 95)
ada_log.fit(Xs1_train, ys1_train)
ys1_predadalog = ada_log.predict(Xs1_test)
print(accuracy_score(ys1_test, ys1_predadalog))

import matplotlib.pyplot as plt
import math
import numpy as np 

x = np.array([1.0, 1.5, 2.0, 2.5, 3.0, 3.25, 3.5, 4.0, 4.5])
y = np.array([0, 0, 10, 40, 70, 80, 100, 100, 100])
plt.plot(x, y, "c")

plt.xlabel("Total rotation speed in seconds")
plt.ylabel("Capture success ratio")

plt.show()

import matplotlib.pyplot as plt
import math
import numpy as np 


x = np.array([0.3, 0.5, 0.6, 0.8, 1.0, 1.08, 1.16, 1.33, 1.5])
y = np.array([0, 0, 10, 40, 70, 80, 100, 100, 100])
plt.plot(x, y)
#plt.plot(x, y, '.', color='blue');


#plt.vlines(x=1.16, ymin=0, ymax=100, color='r', label = 'success')

plt.text(0.6,60,'Failure')

# this text will be right-aligned
plt.text(1.2,60,'Success')

plt.xlabel("Single rotation speed in seconds")
plt.ylabel("Capture success percentage")
plt.savefig('Graph.pdf', dpi=120, format='pdf', bbox_inches='tight')
plt.show()

import matplotlib.pyplot as plt
import math
import numpy as np 

#x = np.linspace(0,1, 100)
#y = np.linspace(0, 255)

with plt.style.context('fivethirtyeight'):
    plt.plot(x, np.array([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100]))
    plt.plot(y, np.array([58,95,104,122,146,45,36,0,72,110,136,84,47,23,133,160,150,177,198,220,225,168,57,16,21,167,135,133,175,112,202,90,188,193,67,97,211,130,42,87,201,119,11,41,57,107,167,209,93,5,51,191,184,170,159,108,88,75,83,64,72,99,127,77,55,15,19,26,89,115,220,217,182,149,215,224,208,193,132,138,172,183,117,185,126,141,179,158,218,176,162,81,69,74,125,71,28,120,189,78]))
    plt.plot(y, np.array([80,117,113,116,156,69,47,77,83,97,142,110,65,58,128,155,146,182,186,214,224,145,38,12,29,167,152,131,173,103,193,102,200,195,55,111,223,137,62,77,206,124,40,45,63,115,175,218,86,10,74,183,167,156,133,114,92,84,90,73,55,107,122,84,63,24,26,33,93,119,223,211,173,148,208,213,200,187,147,142,182,181,119,197,128,154,169,148,224,188,178,88,72,68,121,83,34,127,195,82]))
    

#plt.plot(x, y)

plt.show()

plt.xlabel("Data instances")
plt.ylabel("Number of items in the cabinets")
plt.savefig('Compare.pdf', dpi=120, format='pdf', bbox_inches='tight')
plt.show()

import numpy as np
import matplotlib.pyplot as plt
import matplotlib
matplotlib.rcParams['pdf.fonttype'] = 42
matplotlib.rcParams['ps.fonttype'] = 42
from scipy.stats import norm
import statistics

# Plot between -10 and 10 with .001 steps.
x_axis = np.arange(3, 5, 0.01)

# Calculating mean and standard deviation
mean = statistics.mean(x_axis)

sd = statistics.stdev(x_axis)

plt.plot(x_axis, norm.pdf(x_axis, 4.65, 0.12),  label='Comfortable To Use')
plt.plot(x_axis, norm.pdf(x_axis, 4.45, 0.12),  label='Speed of Transaction')

x = np.array([4, 4, 4, 5])
y = np.array([0, 0, 0, 0])

#plt.scatter(x, y)

plt.xlabel('Mean MOS')
plt.legend()
plt.savefig('MOS.pdf', dpi=120, format='pdf', bbox_inches='tight')

plt.show()