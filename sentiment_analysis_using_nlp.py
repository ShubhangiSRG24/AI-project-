# -*- coding: utf-8 -*-
"""Sentiment analysis using nlp.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FzJ1Pzm_vu43-mzvsTgHUGigcdARbger
"""

pip install -U scikit-learn

import numpy as np
import pandas as pd
import csv

from sklearn.datasets import fetch_20newsgroups

text_data = fetch_20newsgroups(subset='all')

type(text_data)

raw_data = text_data.data #convert the bunch data to list format
raw_data

raw_data1 = text_data.data[:4]
raw_data1

"""#Stage1: convert to lower text"""

lower_text = []

def to_lower_case(data):
  for words in raw_data1:
    lower_text.append(str.lower(words))

to_lower_case(raw_data1)

lower_text

"""#Stage2: Tokenize"""

clean_text_2 = []
from nltk.tokenize import sent_tokenize, word_tokenize

import nltk
nltk.download('punkt')

sent_tok = []
for sent in lower_text:
  sent = sent_tokenize(sent)
  sent_tok.append(sent)

"""#word tokenize"""

clean_text_2 = [word_tokenize(i) for i in lower_text]#list comprehension

clean_text_2

import re #using reg ex get rid of any special characters in the dataset

clean_text_3 = []

for words in clean_text_2:
  clean = []
  for w in words:
    res = re.sub(r'[^\w\s]', '', w)
    if res !='':
      clean.append(w)
    clean_text_3.append(clean)

clean_text_3

"""#Stage 3: Remove stop words aka repeatative words"""

import nltk

nltk.download('stopwords')

from nltk.corpus import stopwords

clean_text_4 = []

for words in clean_text_3:
  w = []
  for word in words:
    if not word in stopwords.words('english'):
      w.append(word)
    clean_text_4.append(w)

clean_text_4

"""# Stage 4: Stemming  aka data preprocess"""

from nltk.stem.porter import PorterStemmer

port = PorterStemmer()

a = [port.stem(i) for i in ["reading", "washing", "wash", "driving"]]
a

clean_text_5 = []

for words in clean_text_4:
  w = []
  for word in words:
    w.append(word)
  clean_text_5.append(w)

clean_text_5

"""# Stage 5: Lemitization (use wordnet) - to avoid stem words that dont make sense"""

from nltk.stem.wordnet import WordNetLemmatizer

wnet = WordNetLemmatizer()

import nltk
nltk.download('wordnet')
nltk.download('omw-1.4')

lem = []

for words in clean_text_4:
  w = []
  for word in words:
    w.append(wnet.lemmatize(word))
  lem.append(w)

lem

X_train = ["This was awesome an awesome movie",
      "Great movie! I liked it a lot",
      "Happy Ending! awesome acting by the hero",
      "loved it! truly great",
      "bad not upto the mark",
      "could have been better",
      "Surely a Disappointing movie"]

y_train = [1,1,1,1,0,0,0] # 1 - Positive, 0 - Negative Class

X_train

"""# data cleaning"""

from nltk.tokenize import RegexpTokenizer

from nltk.stem.porter import PorterStemmer

from nltk.corpus import stopwords

import nltk

nltk.download('stopwords')

tokenizer = RegexpTokenizer(r'\w+')

en_stopwords = set(stopwords.words('english'))

ps = PorterStemmer()

def getCleanedText(text):
  text = text.lower()

  #tokenize
  tokens = tokenizer.tokenize(text)
  new_tokens = [token for token in tokens if token not in en_stopwords]
  stemmed_tokens = [ps.stem(tokens) for tokens in new_tokens]

  clean_text = " ".join(stemmed_tokens)

  return clean_text

X_test = ["I was happy & happy and I loved the acting in the movie",
           "The movie I saw was bad"]

X_Clean = [getCleanedText(i) for i in X_train]  

Xt_Clean = [getCleanedText(i) for i in X_test]

X_Clean

"""#Vectorization"""

from sklearn.feature_extraction.text import CountVectorizer

cv = CountVectorizer(ngram_range=(1,2))

X_vec = cv.fit_transform(X_Clean).toarray()
X_vec

print(cv.get_feature_names_out())

Xt_vect = cv.transform(Xt_Clean).toarray()

"""#multinomial Naive bayes for text classification"""

from sklearn.naive_bayes import MultinomialNB
mn = MultinomialNB()

mn.fit(X_vec, y_train)

y_pred = mn.predict(Xt_vect)
y_pred